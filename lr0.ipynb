{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMd3RlGo70Vm/XYe7ewf/IM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DE50LAT10N/ya-handbook-ml/blob/main/lr0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aieYccmLpfr7"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade git+https://github.com/dask/s3fs --no-deps\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers tokenizers datasets evaluate accelerate --no-deps"
      ],
      "metadata": {
        "id": "RjC9vTSm59FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0039.shtml\",\n",
        "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0040.shtml\",\n",
        "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0050.shtml\",\n",
        "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0060.shtml\",\n",
        "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0070.shtml\",\n",
        "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0080.shtml\",\n",
        "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0090.shtml\",\n",
        "    \"http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_1860_dekabristy.shtml\",\n",
        "]"
      ],
      "metadata": {
        "id": "9wpM-seW5YTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import html\n",
        "import re\n",
        "import requests\n",
        "\n",
        "def download(url):\n",
        "    return requests.get(url).text\n",
        "\n",
        "striptags_re = re.compile(r\"(<!--.*?-->|<[^>]*>)\")\n",
        "entity_re = re.compile(r\"&([^;]+);\")\n",
        "\n",
        "def to_text(s):\n",
        "    return html.unescape(striptags_re.sub(\"\", s))\n",
        "\n",
        "def beautify(s):\n",
        "    lines = [x.strip() for x in s.split(\"\\n\") if x.strip() != \"\"]\n",
        "    for i in range(min(100, len(lines))):\n",
        "        if lines[i] == \"-->\":\n",
        "            break\n",
        "    return \"\\n\".join(lines[i + 1 :] if i < 100 else lines)\n",
        "\n",
        "\n",
        "with open(\"dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for u in urls:\n",
        "        text = beautify(to_text(download(u)))\n",
        "        f.write(text + \"\\n\\n\")"
      ],
      "metadata": {
        "id": "KUPBsNqy6QOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenizers as tok\n",
        "import transformers as tr"
      ],
      "metadata": {
        "id": "_nDlIUO18V_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tok.Tokenizer(tok.models.BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = tok.pre_tokenizers.Whitespace()\n",
        "trainer = tok.trainers.BpeTrainer(special_tokens=[\"[PAD]\"])\n",
        "tokenizer.train([\"dataset.txt\"], trainer)\n",
        "tokenizer.enable_padding()"
      ],
      "metadata": {
        "id": "6V0e4BJmAlW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenizer.get_vocab()\n",
        "ttokenizer = tr.PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
        "len(vocab)"
      ],
      "metadata": {
        "id": "tTl9WjZGAgB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"text\", data_files=\"dataset.txt\")\n",
        "dataset[\"train\"][13]"
      ],
      "metadata": {
        "id": "0ChKeoWP-IVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(x):\n",
        "    x = ttokenizer(x[\"text\"])\n",
        "    x[\"labels\"] = x[\"input_ids\"].copy()\n",
        "    return x\n",
        "\n",
        "\n",
        "ds = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "ds[\"train\"][0]"
      ],
      "metadata": {
        "id": "gsbhuP_BAXiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "block_size = 1024\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "dsb = ds.map(group_texts, batched=True)"
      ],
      "metadata": {
        "id": "lQ7kSWeK_5F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tr.AutoTokenizer.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
        "gpt = tr.GPT2LMHeadModel.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
        "res = gpt.generate(\n",
        "    **tokenizer(\"Мне нравится, что вы \", return_tensors=\"pt\"),\n",
        "    max_new_tokens=50,\n",
        "    top_k=3,\n",
        "    do_sample=True\n",
        ")\n",
        "tokenizer.decode(res[0])"
      ],
      "metadata": {
        "id": "0CctyDob-xJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\"text\", data_files=\"dataset.txt\")\n",
        "ds = dataset.map(lambda x:\n",
        "                 tokenizer(x[\"text\"]), batched=True, remove_columns=[\"text\"])\n",
        "dsb = ds.map(group_texts, batched=True)"
      ],
      "metadata": {
        "id": "XLpCYtaU-zQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "d6lU5XHGB3zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targs = tr.TrainingArguments(\n",
        "    output_dir=\"gpt2-finetune\",\n",
        "    num_train_epochs=30,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=200,\n",
        "    save_steps=1500,\n",
        ")\n",
        "trainer = tr.Trainer(\n",
        "    gpt,\n",
        "    args=targs,\n",
        "    train_dataset=dsb[\"train\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=tr.default_data_collator,  # tr.DataCollatorForLanguageModeling(tokenizer=ttokenizer,mlm=False)\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "vcEwcIg4-13O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = gpt.generate(\n",
        "    **tokenizer(\"Мне нравится, что вы \", return_tensors=\"pt\").to(\"cuda\"),\n",
        "    max_new_tokens=50,\n",
        "    top_k=3,\n",
        "    do_sample=True\n",
        ")\n",
        "tokenizer.decode(res[0])"
      ],
      "metadata": {
        "id": "ihA8zR---3bn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}